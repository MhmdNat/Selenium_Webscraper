import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import io
from PIL import Image
import time
# PATH = "C:\\Users\\aliha\OneDrive\\Desktop\\chromedriver-win32\\chromedriver-win32"
# service = Service(PATH)
# wd = webdriver.Chrome(service=service)
wd = webdriver.Chrome()
def get_urls_from_google(wd, delay, max_images):
    
    
    def scroll_down(wd):
        wd.execute_script("window.scrollTo(0, document.body.scrollHeight);") 
        time.sleep(delay)
    image_urls = set()
    maxpage = max_images // 2
    total_images = 0
    
    try:
        for j in range(1, 3):
            url = f"https://www.inaturalist.org/observations?page={j}&quality_grade=research&taxon_id=987322"
            print(f"Processing page {j}...")
            wd.get(url)
            time.sleep(2)  
            
            i = 1
            page_images = 0
            stop = False
            
            while page_images < maxpage and not stop:
                scroll_down(wd)
                thumbnails = wd.find_elements(By.CSS_SELECTOR, ".photo.has-photo")      
                for img in thumbnails:
                    if stop or total_images >= max_images:
                        stop = True
                        break
                        
                    try:
                        img.click()
                        time.sleep(delay)
                        
                        try:
                            image_elements = wd.find_elements(By.CSS_SELECTOR, '.image-gallery-image img')
                            for image in image_elements:
                                if page_images >= maxpage:
                                    stop = True
                                    break
                                    
                                src = image.get_attribute('src')
                                if src and 'http' in src and src not in image_urls:
                                    image_urls.add(src)
                                    page_images += 1
                                    total_images += 1
                                    print(f"Image added! page = {j}, photo nb = {total_images}")
                                    
                        except Exception as e:
                            print(f"Error processing image: {e}")
                            
                        finally:
                            wd.back()
                            time.sleep(delay)
                            
                    except Exception as e:
                        print(f"Error clicking thumbnail: {e}")
                        continue
                        
            print(f"Completed page {j} with {page_images} images")
            
    except Exception as e:
        print(f"Error during scraping: {e}")
        
    print(f"Total unique images found: {len(image_urls)}")
    return image_urls
   
def download_image(download_path, url, file_name):
    try:
        image_content = requests.get(url).content 
        image_file = io.BytesIO(image_content)
        image = Image.open(image_file) 
        file_path = download_path + file_name 
        with open(file_path, "wb") as f:
            image.save(f, "JPEG") 
        print("success")
        
    except Exception as e:
        print("failed -", e)    
#Main
max_photos = 192
urls = get_urls_from_google(wd, 1, max_photos)
for i, url in enumerate(urls):
    download_image("C:\\Users\\aliha\OneDrive\\Desktop\\fallE2\\VIP\\week 2-3\\photosdownloaded\\",url, f"{i}.jpg")
